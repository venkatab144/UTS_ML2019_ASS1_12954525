{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_A1_12954525.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "60cuvgukRncT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatab144/UTS_ML2019_ASS1_12954525/blob/master/ML_A1_12954525.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYWl-5S3n3QD",
        "colab_type": "text"
      },
      "source": [
        "# Notes\n",
        "\n",
        "http://yann.lecun.com/exdb/publis/pdf/bottou-05.pdf\n",
        "\n",
        "https://engmrk.com/lenet-5-a-classic-cnn-architecture/\n",
        "\n",
        "https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17\n",
        "\n",
        "https://en.wikipedia.org/wiki/Convolutional_neural_network#Design\n",
        "\n",
        "https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d#e276\n",
        "\n",
        "https://dataconomy.com/2017/04/history-neural-networks/\n",
        "\n",
        "https://medium.com/@pechyonkin/key-deep-learning-architectures-lenet-5-6fc3c59e6f4\n",
        "\n",
        "https://colab.research.google.com/drive/1CVm50PGE4vhtB5I_a_yc4h5F-itKOVL9#scrollTo=1w66ueiLlP0k&forceEdit=true&offline=true\n",
        "\n",
        "https://medium.com/@vijendra1125/alexnet-overview-75880645b14c\n",
        "\n",
        "https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\n",
        "\n",
        "https://en.wikipedia.org/wiki/AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60cuvgukRncT",
        "colab_type": "text"
      },
      "source": [
        "# Draft and Experiment Area"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpBLaxOGRncU",
        "colab_type": "text"
      },
      "source": [
        "1. First impression\n",
        "    * What is my chosen paper to read?\n",
        "        - Lec98 - Convolutional Nueral Networks\n",
        "    * What type of the main contribution the paper has made?\n",
        "        - A theory or proposition (revealing something, from unknown to known)\n",
        "        - A method or algorithm (inventing a technique, from undoable to doable)\n",
        "\n",
        "    * _Before_ reading the main body of the paper, write down your first impression  obtained from its abstract and short introduction.\n",
        "    * Why does the paper attract you, such as, How it surprised you? Why do you think it addresses an important topic that will be helpful in your future study of machine learning?\n",
        "    \n",
        "2. Read the paper abstract and introduction, list here all the notions that you don't know the precise meaning. If you think you have completed your list,  compare the list with people around you who have chosen the same or a similar paper.\n",
        "\n",
        "3. (During the next 7 days) Re-consider the central problem of the paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_L8AZXERncV",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"[LEC98] Convolutional Neural Network\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUOw1N3gRncW",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Convolutional Neural Networks (CNN) are Deep Learning Algorithms that are used primarily for pattern recognition and visual imagery analysis. They are comprised of various layers such the: input and output layers and many different hidden layers. Hidden layers within a CNN often contain convolution layers, pooling layers, normalization layers and fully connected layers. Hidden layers are located in between the input and output layers and use artificial neurons to take input, and through some activation function, produce an output - this is the feature extraction phase of the CNN. These inputs and outputs (of the hidden layers) are not observed by the training set and are masked or \"hidden\" by the activation function, hence the name \"hidden\" layers. The activation function is most commonly a RELU (Rectified Linear Unit) layer, but, at the time this paper was published, RELU was not used, the first notable use of RELU was in AlexNet (2012)[S]. Instead, LeNet-5 - the CNN architecture presented in this paper, utilizes a sigmoid squashing function [PUT FUNCTION HERE]. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yctSlqZARncW",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNX-0Rx8RncX",
        "colab_type": "text"
      },
      "source": [
        "The research is about .... A theorem has been proved stating ... / An algorithm of ... has been proposed.\n",
        "\n",
        "The paper \"Gradient Based Learning Applied to Document\n",
        "Recognition\" addresses the problem with the current approach, at the time of publishing, of single digit Optical Character Recognition (OCR). The problem highlighted by the paper for the existing method is \"that the recognition accuracy is largely determined by the ability of the designer to come up with an appropriate set of features\"[S]\n",
        "\n",
        "The main objective of the paper is to describe and build a CNN architecture for a better pattern recognition system, that employs a more automatic learning approach to pattern recognition. The existing methods used a combination of modules: a feature extraction module and a classifier module as seen in figure 1, in order to isolate characters and transform them into patterns so that they can be compared to a set of pre-determined characters and classified accordingly.\n",
        "\n",
        "The paper uses the MNIST dataset which comprises of a series of handwritten digits, in order to show that the traditional model for recognition systems can be replaced by the Graph Transformer Network (GTN) design paradigm. The paper uses various different learning algorithms and compares their performance and accuracy using the MNIST dataset as a benchmark for hand written digit recognition. Gradient-Based Learning is the procedure that is used to modify the parameters of the function in order the minimise the error for the global loss function. In order to minimise the error for the global loss function, a back-propogation algorithm is used to compute the gradient of the loss function with respect to all parameters being used for the algorithm. Gradient-Based Learning is used to modify the parameters of the system using the calculated gradients, and using Gradient Back-Propogation, it is possible to identify the ideal parameters, weights and biases for the algorithm.\n",
        "\n",
        "The solution proposed by the paper is the LeNet-5 CNN Digit Recognition Architecture. The paper continues on to address the segmentation and recognition of multiple combined objects, in this context; multiple characters, by using a Graph Transformer Network and a Viterbi Transformer to analyse each character and produce a graph with a singular path which has the lowest Viterbi Penalty, this represents the best interpretation of the multiple characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M3gu9NbRncY",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWXqj4p_RncY",
        "colab_type": "text"
      },
      "source": [
        "The background at the time of the work is that people understood the problem as .... The creative idea is ...\n",
        "\n",
        "The paper proposed the idea of creating a CNN architecture that relied less on hand-crafted heuristics in order to isolate feature sets, and instead relied more on automatic learning and convolutions that allowed the architecture to use less memory (measured by number of variables) and perform less multiply-accumulate operations on the CPU[APPR IMG], by storing learnable parameters. GPU's were not used at that time to help with training and instead the workload was place on CPU's which were considerably slower.\n",
        "\n",
        "LeNet-5 consists of 8 layers:\n",
        "1.   Input Layer: takes a 32x32 image.\n",
        "2.   Convolutional Layer C1: 6 feature maps at 28x28, 5x5 receptive fields are used with a Stride length of 1.\n",
        "3.   Subsampling (Pooling) Layer S2: Average pooling is applied and receptive fields with Stride length 2 are used to create 6 feature maps of size 14x14.\n",
        "4.   Convolutional Layer C3: Further convolusions are applied and 16 feature maps at 10x10 are created. 10 of the 16 feature maps generated in this layer are combined with the 6 feature maps generated in the S2 layer at identical locations [TABLE 1].\n",
        "5.   Subsampling (Pooling) Layer S4: Same as S2, produces 16 feature maps at 5x5.\n",
        "6.   Fully Connected Convolutional Layer C5: Has 120 feature maps of size 1x1.\n",
        "7.   Fully Connected Layer F6: 84 neurons. Every layer (excluding the input layer) up to F6 are passed through an activation function. But first the dot product of all the layers is computed for the input vector and its associated weight vector, and the appropriate bias, which was calculated using Gradient Back-Propagation, is added. The activation function for these layers is a sigmoid squashing function defined in [FORMULA]. The calculated dot product [aSUBi] is passed through this activation function for the given unit [i]. \n",
        "8.   Output Layer: The output layer is another fully connected layer with 10 possible values representing the digits from 0-9. The activation function for this layer is the Euclidean Radial Basis Function (RBF) [FROMULA RBF]\n",
        "\n",
        "The paper also discusses the problem of segmenting and recognising multiple objects stringed together such as: words, phone numbers, zip codes and check amounts (which was what LeNet-5 was commonly used for). It achieves this using Graph Transformer Networks and the Viterbi Transfomer which is based on the Viterbi Algorithm. The purpose of the Viterbi algorithm is to select the generated graphs with the lowest Viterbi Penalty, giving the best interpretation. The basic architecture provided in this paper [ARCH GTN], makes use of two graph transformers: recognition transformer and the viterbi transformer. The recognition transformer is responsible for analyzing the un-segmented object and using heuristic over-segmentation to generate all the possible segmentations for the given un-segmented input. The viterbi transformer takes all the possible interpretations of all the segmentations from the recognition transformer and computes the Viterbi path with the lowest Viterbi penalty. Each Viterbi path contains a start node, an arc and an end node. The arc holds the labels that represent a digit, analyzing the arcs from the graph obtained from the viterbi transformer will provide the results of the interpretation.\n",
        "\n",
        "The LeNet-5 architeture was significant at the time of publishing as it achieved an error rate of 0.70%[ERROR RATE]. This architecture was one of the very first convolusional neural networks during its creation and its utilization of stacked convolution layers, pooling layers and the final fully connect layer containing the classifier, became a standard for future CNN architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG2nh1hBRncZ",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s0lpF2BRnca",
        "colab_type": "text"
      },
      "source": [
        "The technical development if of high/low quality. The authors supported their theory using ...\n",
        "\n",
        "The technical quality of the paper is very high, according to Google, it has been cited more than 20320 times. The techincal findings in this paper accompanied with in-depth explanation of all key components including the relevant math. The solution provided by the paper, and the information and innovation in the paper itself, laid the groundwork for future CNN architecutres such as AlexNet, which is regarded as one of the most influential papers published in the Computer Vision field.\n",
        "\n",
        "The results and architectures proposed within the paper are easily replicable using Python and modern day machine learning libraries. The paper provides all the necessary detail and information required to be able to replicate the architecture and produce the same results yourself. The proposed solution in this paper was compared with other implementations of solutions that address the same issue of hand-written character recognition. \n",
        "\n",
        "The conclusions drawn from the paper are from the real world implementation of the LeNet-5 architecture, which, at the time, was being used by banks to identify the monetary amount on checks. The architecture was also tested against the MNIST dataset which contains more than 60,000 training patterns. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VXBXHXlRncb",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcNK1HNkRncb",
        "colab_type": "text"
      },
      "source": [
        "I find the proposal in the paper promising. ...\n",
        "\n",
        "The proposed ideas in the paper were extremely promising at the time, especially in a field that was still in its early stages of inception. The notion of machine learning did not draw a lot of interest from the community during the publishing of the paper as stated by Yann LeCun himself: “There was a dark period between the mid-90s and early-to-mid-2000s when it was impossible to publish research on neural nets, because the community had lost interest in it,” says LeCun. “In fact, it had a bad rep. It was a bit taboo.”[VERGE]. \n",
        "\n",
        "The LeNet architecture described in the paper is still widely used today and forms the basis of many CNN designs OCR systems around the world. The application domain of the paper is appropriate for the techniques described. It was one of the first CNN designs created and help pioneer its domain (Machine Learning/OCR).\n",
        "\n",
        "The concept of visual pattern recognition to classify handwritten characters can be further expanded upon, to the application domain of facial recognition or just general image recognition. The same design paradigms can be used to extract feature sets from images and perform classification tasks. In fact, Yan LeCun, the author, is the Chief AI Scientist at Facebook, which involves facial and image recognition. LeCun also notably recieved the Turing award for his contributions to the Deep Learning field in 2018.\n",
        "\n",
        "The work described in the paper can most definately spark discussion in class. The paper explains all the concepts in a way that anyone with basic knowledge in machine learning will be able to follow. The mathematical concepts and formulae present in the paper, however, are a bit harder to grasp. A fundamental knowledge of the ideas presented in the paper would be extremely beneficial if taught to individuals who are interested in the machine learning field but only possess a basic knowledge. It is a good starting step for anyone looking at the field to form a basic understanding of the designs and architecture behind CNN and other Deep Learning principles. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mywlhk3TRncc",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwrUiPJjRnce",
        "colab_type": "text"
      },
      "source": [
        "The overall strucutre is clear. I found reading is easy / difficult. The paper could have been more attractive if the authors had organised ... / provided ... "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wybpRC_DRncf",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "[SHA48][1]: Author, Title, Info\n",
        "\n",
        "[1]:https://google.com"
      ]
    }
  ]
}